
MileStone
shubham kumar
7/17/2020
Instructions
The first step in building a predictive model for text is understanding the distribution and relationship between the words, tokens, and phrases in the text. The goal of this task is to understand the basic relationships you observe in the data and prepare to build your first linguistic models.

The goal of this project is just to display that you’ve gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager.

You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to: 1. Demonstrate that you’ve downloaded the data and have successfully loaded it in. 2. Create a basic report of summary statistics about the data sets. 3. Report any interesting findings that you amassed so far. 4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

Tasks to accomplish
Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.
getwd()
## [1] "C:/Users/gabre.a.hughes/Documents/JHU/Capstone/Mile"
path <- getwd()
{
  download.file('https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip',
                destfile = paste(path, 'Coursera-SwiftKey.zip',  sep = "/"))
unzip('./Coursera-SwiftKey.zip')
}
library(stringr)
## Warning: package 'stringr' was built under R version 3.6.3
fileNames <- list.files("final/en_US", full.names=TRUE)
totalLines <- NULL
longestLine <- NULL
size <- NULL
totalWords <- NULL
aveWords <- NULL
for (i in 1:length(fileNames)) {
    conn <- file(fileNames[i], open = "rb")
    file <- readLines(conn, skipNul = TRUE)
    totalLines <- c(totalLines, length(file))
    longestLine <- c(longestLine, max(nchar(file)))
    size <- c(size, round(file.size(fileNames[i])/1024/1024, 2))
    totalWords <- c(totalWords, sum(str_count(file, '\\w+')))
    aveWords <- c(aveWords, round(mean(str_count(file, '\\w+')), 2))
    close(conn)
}
rm(file)
filestats <- as.data.frame(cbind(fileNames, size, totalLines, longestLine, totalWords, aveWords))
library(knitr)
## Warning: package 'knitr' was built under R version 3.6.3
kable(filestats, col.names = c("File Names", "Size in MB", "Total Lines", "Longest Line", "Total Words", "Average Words per Line"))
File Names	Size in MB	Total Lines	Longest Line	Total Words	Average Words per Line
final/en_US/en_US.blogs.txt	200.42	899288	40835	38601176	42.92
final/en_US/en_US.news.txt	196.28	1010242	11384	35806831	35.44
final/en_US/en_US.twitter.txt	159.36	2360148	213	31130623	13.19
suppressMessages(library(quanteda))
## Warning: package 'quanteda' was built under R version 3.6.3
suppressMessages(library(wordcloud))
## Warning: package 'wordcloud' was built under R version 3.6.3
suppressMessages(library(ggplot2))
par(mfrow=c(1, 3))
#Generate and plot twitter dfm
twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul = TRUE)
set.seed(123)
sample <- as.logical(rbinom (n=length(twitter),size=1, prob = 0.10))
sampleTweets <- twitter[sample]
rm(twitter)
twitterDFM <- dfm(sampleTweets, verbose = FALSE, ignoredFeatures=stopwords("english"))
## Warning: ignoredFeatures argument is not used.
suppressMessages(wordcloud(names(topfeatures(twitterDFM, 100)), topfeatures(twitterDFM,100), colors="steelblue3"))
#Generate and plot blog dfm
blogs <- readLines("final/en_US/en_US.blogs.txt")
sample <- as.logical(rbinom (n=length(blogs),size=1, prob = 0.10))
sampleBlogs <- blogs[sample]
rm(blogs)
blogsDFM <- dfm(sampleBlogs, verbose = FALSE, ignoredFeatures=stopwords("english"))
## Warning: ignoredFeatures argument is not used.
suppressMessages(wordcloud(names(topfeatures(blogsDFM, 100)), topfeatures(blogsDFM,100), colors="darkolivegreen4"))
#Generate and plot news dfm
conn <- file("final/en_US/en_US.news.txt", open = "rb")
news <- readLines(conn, skipNul = TRUE)
sample <- as.logical(rbinom (n=length(news),size=1, prob = 0.10))
sampleNews <- news[sample]
rm(news)
rm(sample)
newsDFM <- dfm(sampleNews, verbose = FALSE, ignoredFeatures=stopwords("english"))
## Warning: ignoredFeatures argument is not used.
suppressMessages(wordcloud(names(topfeatures(newsDFM, 100)), topfeatures(newsDFM,100), colors="coral3"))


suppressMessages(twitterBigram <- dfm(sampleTweets, ngrams=2))
## Warning: ngrams argument is not used.
topTwitter <- data.frame(frequency=topfeatures(twitterBigram,30), row.names = NULL)
topTwitter$ngram <- names(topfeatures(twitterBigram,30))
par(mfrow=c(1,1))
barplot(topTwitter$frequency, names.arg = topTwitter$ngram, xlab = "Bigrams", ylab = "Frequency", main="Top Twitter Bigrams", col="steelblue", cex.axis=0.3)


suppressMessages(BlogBigram <- dfm(sampleBlogs, ngrams=2))
## Warning: ngrams argument is not used.
topBlog <- data.frame(frequency=topfeatures(BlogBigram,30), row.names = NULL)
topBlog$ngram <- names(topfeatures(BlogBigram,30))
barplot(topBlog$frequency, names.arg = topBlog$ngram, xlab = "Bigrams", ylab = "Frequency", main="Top Blogs Bigrams", col="darkolivegreen4", cex.axis=0.3)


suppressMessages(NewsBigram <- dfm(sampleNews, ngrams=2))
## Warning: ngrams argument is not used.
topNews <- data.frame(frequency=topfeatures(NewsBigram,30), row.names = NULL)
topNews$ngram <- names(topfeatures(NewsBigram,30))
barplot(topNews$frequency, names.arg = topNews$ngram, xlab = "Bigrams", ylab = "Frequency", main="Top News Bigrams", col="coral3", cex.axis=0.3)


news <- readLines("final/en_US/en_US.news.txt")
## Warning in readLines("final/en_US/en_US.news.txt"): incomplete final line found
## on 'final/en_US/en_US.news.txt'
library(stringr)
fileNames <- list.files("final/en_US", full.names=TRUE)
totalLines <- NULL
longestLine <- NULL
size <- NULL
totalWords <- NULL
aveWords <- NULL
for (i in 1:length(fileNames)) {
    conn <- file(fileNames[i], open = "rb")
    file <- readLines(conn, skipNul = TRUE)
    totalLines <- c(totalLines, length(file))
    longestLine <- c(longestLine, max(nchar(file)))
    size <- c(size, round(file.size(fileNames[i])/1024/1024, 2))
    totalWords <- c(totalWords, sum(str_count(file, '\\w+')))
    aveWords <- c(aveWords, round(mean(str_count(file, '\\w+')), 2))
    close(conn)
}
rm(file)
filestats <- as.data.frame(cbind(fileNames, size, totalLines, longestLine, totalWords, aveWords))
library(knitr)
kable(topB, col.names = c("File Names", "Size in MB", "Total Lines", "Longest Line", "Total Words", "Average Words per Line"))
suppressMessages(library(quanteda))
suppressMessages(library(wordcloud))
suppressMessages(library(ggplot2))
par(mfrow=c(1, 3))
#Generate and plot twitter dfm
twitter <- readLines("final/en_US/en_US.twitter.txt", skipNul = TRUE)
set.seed(123)
sample <- as.logical(rbinom (n=length(twitter),size=1, prob = 0.10))
sampleTweets <- twitter[sample]
rm(twitter)
twitterDFM <- dfm(sampleTweets, verbose = FALSE, ignoredFeatures=stopwords("english"))
suppressMessages(wordcloud(names(topfeatures(twitterDFM, 100)), topfeatures(twitterDFM,100), colors="steelblue3"))
#Generate and plot blog dfm
blogs <- readLines("final/en_US/en_US.blogs.txt")
sample <- as.logical(rbinom (n=length(blogs),size=1, prob = 0.10))
sampleBlogs <- blogs[sample]
rm(blogs)
blogsDFM <- dfm(sampleBlogs, verbose = FALSE, ignoredFeatures=stopwords("english"))
suppressMessages(wordcloud(names(topfeatures(blogsDFM, 100)), topfeatures(blogsDFM,100), colors="darkolivegreen4"))
#Generate and plot news dfm
conn <- file("final/en_US/en_US.news.txt", open = "rb")
news <- readLines(conn, skipNul = TRUE)
sample <- as.logical(rbinom (n=length(news),size=1, prob = 0.10))
sampleNews <- news[sample]
rm(news)
rm(sample)
newsDFM <- dfm(sampleNews, verbose = FALSE, ignoredFeatures=stopwords("english"))
suppressMessages(wordcloud(names(topfeatures(newsDFM, 100)), topfeatures(newsDFM,100), colors="coral3"))
suppressMessages(twitterBigram <- dfm(sampleTweets, ngrams=2))
topTwitter <- data.frame(frequency=topfeatures(twitterBigram,30), row.names = NULL)
topTwitter$ngram <- names(topfeatures(twitterBigram,30))
par(mfrow=c(1,1))
barplot(topTwitter$frequency, names.arg = topTwitter$ngram, xlab = "Bigrams", ylab = "Frequency", main="Top Twitter Bigrams", col="steelblue", cex.axis=0.3)
suppressMessages(BlogBigram <- dfm(sampleBlogs, ngrams=2))
topBlog <- data.frame(frequency=topfeatures(BlogBigram,30), row.names = NULL)
topBlog$ngram <- names(topfeatures(BlogBigram,30))
barplot(topBlog$frequency, names.arg = topBlog$ngram, xlab = "Bigrams", ylab = "Frequency", main="Top Blogs Bigrams", col="darkolivegreen4", cex.axis=0.3)
suppressMessages(NewsBigram <- dfm(sampleNews, ngrams=2))
topNews <- data.frame(frequency=topfeatures(NewsBigram,30), row.names = NULL)
topNews$ngram <- names(topfeatures(NewsBigram,30))
barplot(topNews$frequency, names.arg = topNews$ngram, xlab = "Bigrams", ylab = "Frequency", main="Top News Bigrams", col="coral3", cex.axis=0.3)
Conclusions and discussions
Articles and prepositions are some of the most frequenctly
If the stop words are removed first, verbs such as say, like, go, get are among the most frequently used words
Some words in different tenses are double/triple-counted, as in the case of say/said. This could be something worth exploring to figure out how to view words in different tenses as the same
Similarly, it could be interesting to explore homonyms and separate them by contexts
As the n for n grams increase, the computation time and size dramatically increases
1 gram 50.1MB; 2 gram 1.2 GB; 3 gram 4.4 GB
It would be important to think about parallelization in R to reduce run time
Considering all words, the most frequently used 26 words constitutes 90% of usage; if stop words are removed, still only 183 words are required!
Future plans
Explore parallelization in R
Build basic n-gram model with Markov Chains
Inluce 4 grams if my computer can handle it
Calculate % of all 2 and 3 grams used. When predciting, return the most frequent regardless of n grams
